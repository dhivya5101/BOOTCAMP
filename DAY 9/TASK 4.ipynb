{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20c61efc-df80-4f96-b887-24270e42bb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in current directory: ['# Example code using PyTorch.py', '.anaconda', '.android', '.cache', '.conda', '.condarc', '.continuum', '.gitconfig', '.idlerc', '.ipfs', '.ipynb_checkpoints', '.ipython', '.jupyter', '.kaggle', '.keras', '.matplotlib', '.VirtualBox', '.vscode', '.wdm', '.yarnrc', '1.NLP Intro.pdf', '2.Language Model.pdf', '3D Objects', '4.Tokenization.pdf', '9.NLTK.pdf', 'Advertising.csv', 'anaconda3', 'anaconda_projects', 'AppData', 'Application Data', 'archive (1).rar', 'bonus task.ipynb', 'BONUS.ipynb', 'bottom.jsx', 'Brochure.pdf', 'car.csv', 'cat.0.jpg', 'cat.1.jpg', 'cat.10.jpg', 'cat.11.jpg', 'cat.2.jpg', 'cat.3.jpg', 'cat.4.jpg', 'cat.5.jpg', 'cat.6.jpg', 'cat.7.jpg', 'cat.8.jpg', 'cat.9.jpg', 'Contacts', 'Cookies', 'customer_churn_data.csv', 'data', 'data.txt', 'Desktop', 'dhivya.img', 'Documents', 'dog.0.jpg', 'dog.1.jpg', 'dog.10.jpg', 'dog.11.jpg', 'dog.2.jpg', 'dog.3.jpg', 'dog.4.jpg', 'dog.5.jpg', 'dog.6.jpg', 'dog.7.jpg', 'dog.8.jpg', 'dog.9.jpg', 'Downloads', 'faiss_index', 'fake news detection', 'fake news detection.ipynb', 'fake-news-detection.zip', 'Favorites', 'feature_impact_report.csv', 'genre_labels.npy', 'genymotion-logs-20230523-091943.zip', 'genymotion-logs-20230523-092039.zip', 'genymotion-logs-20230523-103830.zip', 'genymotion-logs-20230523-104427.zip', 'genymotion-logs-20230524-093432.zip', 'genymotion-logs-20230524-093654.zip', 'genymotion-logs-20230524-094723.zip', 'GT_vs_RR_Final_Ball_by_Ball.csv', 'import math.py', 'IntelGraphicsProfiles', 'IPL_Final_Ball_by_Ball.csv', 'linear_regression_model.pkl', 'Links', 'llama-2-7b-chat.ggmlv3.q4_K_M.bin', 'Local Settings', 'middle.jsx', 'Music', 'music_genre_model.h5', 'My Documents', 'NetHood', 'node_modules', 'npm', 'NTUSER.DAT', 'ntuser.dat.LOG1', 'ntuser.dat.LOG2', 'NTUSER.DAT{53b39e88-18c4-11ea-a811-000d3aa4692b}.TM.blf', 'NTUSER.DAT{53b39e88-18c4-11ea-a811-000d3aa4692b}.TMContainer00000000000000000001.regtrans-ms', 'NTUSER.DAT{53b39e88-18c4-11ea-a811-000d3aa4692b}.TMContainer00000000000000000002.regtrans-ms', 'ntuser.ini', 'OneDrive', 'package-lock.json', 'package.json', 'Pictures', 'PrintHood', 'quikr_car.xlsx', 'react.js', 'react.js App', 'ReadAndWrite', 'Recent', 'resume.....pdf', 'Saved Games', 'scikit_learn_data', 'Searches', 'SendTo', 'Start Menu', 'Student Performance Dataset.csv', 'task  2.py', 'task 1', 'TASK 1.ipynb', 'Task 1.py', 'TASK 2.ipynb', 'task 2.py', 'TASK 3.ipynb', 'task 3.py', 'TASK 4.ipynb', 'TASK 5.ipynb', 'Templates', 'thyroid_cancer_risk_data.csv', 'Todo app.js App', 'top.jsx', 'top50.csv', 'top50_utf8.csv', 'train_news.csv', 'Untitled.ipynb', 'untitled.py', 'untitled.txt', 'Untitled1.ipynb', 'untitled1.py', 'untitled1.txt', 'Untitled10.ipynb', 'Untitled100.ipynb', 'Untitled101.ipynb', 'Untitled102.ipynb', 'Untitled103.ipynb', 'Untitled104.ipynb', 'Untitled105.ipynb', 'Untitled106.ipynb', 'Untitled107.ipynb', 'Untitled108.ipynb', 'Untitled109.ipynb', 'Untitled11.ipynb', 'Untitled12.ipynb', 'Untitled13.ipynb', 'Untitled14.ipynb', 'Untitled15.ipynb', 'Untitled16.ipynb', 'Untitled17.ipynb', 'Untitled18.ipynb', 'Untitled19.ipynb', 'Untitled2.ipynb', 'untitled2.py', 'untitled2.txt', 'Untitled20.ipynb', 'Untitled21.ipynb', 'Untitled22.ipynb', 'Untitled23.ipynb', 'Untitled24.ipynb', 'Untitled25.ipynb', 'Untitled26.ipynb', 'Untitled27.ipynb', 'Untitled28.ipynb', 'Untitled29.ipynb', 'Untitled3.ipynb', 'untitled3.txt', 'Untitled30.ipynb', 'Untitled31.ipynb', 'Untitled32.ipynb', 'Untitled33.ipynb', 'Untitled34.ipynb', 'Untitled35.ipynb', 'Untitled36.ipynb', 'Untitled37.ipynb', 'Untitled38.ipynb', 'Untitled39.ipynb', 'Untitled4.ipynb', 'Untitled40.ipynb', 'Untitled41.ipynb', 'Untitled42.ipynb', 'Untitled43.ipynb', 'Untitled44.ipynb', 'Untitled45.ipynb', 'Untitled46.ipynb', 'Untitled47.ipynb', 'Untitled48.ipynb', 'Untitled49.ipynb', 'Untitled5.ipynb', 'Untitled50.ipynb', 'Untitled51.ipynb', 'Untitled52.ipynb', 'Untitled53.ipynb', 'Untitled54.ipynb', 'Untitled55.ipynb', 'Untitled56.ipynb', 'Untitled57.ipynb', 'Untitled58.ipynb', 'Untitled59.ipynb', 'Untitled6.ipynb', 'Untitled60.ipynb', 'Untitled61.ipynb', 'Untitled62.ipynb', 'Untitled63.ipynb', 'Untitled64.ipynb', 'Untitled65.ipynb', 'Untitled66.ipynb', 'Untitled67.ipynb', 'Untitled68.ipynb', 'Untitled69.ipynb', 'Untitled7.ipynb', 'Untitled70.ipynb', 'Untitled71.ipynb', 'Untitled72.ipynb', 'Untitled73.ipynb', 'Untitled74.ipynb', 'Untitled75.ipynb', 'Untitled76.ipynb', 'Untitled77.ipynb', 'Untitled78.ipynb', 'Untitled79.ipynb', 'Untitled8.ipynb', 'Untitled80.ipynb', 'Untitled81.ipynb', 'Untitled82.ipynb', 'Untitled83.ipynb', 'Untitled84.ipynb', 'Untitled85.ipynb', 'Untitled86.ipynb', 'Untitled87.ipynb', 'Untitled88.ipynb', 'Untitled89.ipynb', 'Untitled9.ipynb', 'Untitled90.ipynb', 'Untitled91.ipynb', 'Untitled92.ipynb', 'Untitled93.ipynb', 'Untitled94.ipynb', 'Untitled95.ipynb', 'Untitled96.ipynb', 'Untitled97.ipynb', 'Untitled98.ipynb', 'Untitled99.ipynb', 'vector_store', 'Videos', 'VirtualBox VMs', 'WALLPAPER.jpg']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List files in the current directory\n",
    "print(\"Files in current directory:\", os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "719cb16c-0357-4db2-b497-a7056872f261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files moved to 'data' folder successfully!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Create 'data' folder if it doesn't exist\n",
    "destination_folder = \"data\"\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "\n",
    "# Move PDF files\n",
    "for file in os.listdir():\n",
    "    if file.endswith(\".pdf\"):  # Ensure only PDFs are moved\n",
    "        shutil.move(file, os.path.join(destination_folder, file))\n",
    "\n",
    "print(\"Files moved to 'data' folder successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79eb35f2-7f7a-4225-9aa6-453e2642cd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs in data folder: ['.ipynb_checkpoints', '1.NLP Intro.pdf', '2.Language Model.pdf', '4.Tokenization.pdf', '9.NLTK.pdf']\n"
     ]
    }
   ],
   "source": [
    "print(\"PDFs in data folder:\", os.listdir(\"data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b2dad37-6cba-4365-a21f-705c44656ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.3.19)\n",
      "Requirement already satisfied: pypdf in c:\\users\\hp\\anaconda3\\lib\\site-packages (5.3.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (0.3.40)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (2.10.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (2.32.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.49.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.15.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.29.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from faiss-cpu) (23.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain pypdf sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b71092ea-1237-48b4-9f65-3e50856c113b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS vector store saved in 'vector_store'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Step 1: Load PDF Documents from 'data' Folder\n",
    "data_folder = \"data\"  \n",
    "loader = DirectoryLoader(data_folder, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 2: Split Documents into Chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Step 3: Create Embeddings using HuggingFace\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 4: Build FAISS Vector Store\n",
    "vector_store = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "# Step 5: Save Vector Store to a Folder\n",
    "faiss_folder = \"vector_store\"\n",
    "vector_store.save_local(faiss_folder)\n",
    "\n",
    "print(f\"FAISS vector store saved in '{faiss_folder}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76b4a4ae-3b0f-4258-a1c4-bac699205a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store Files: ['index.faiss', 'index.pkl']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "faiss_folder = \"vector_store\"\n",
    "print(\"Vector Store Files:\", os.listdir(faiss_folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91a99320-b175-4ba0-98ac-80bc504b360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Query: What is NLP?\n",
      "\n",
      " Match 1:\n",
      "2/919AD601 - Origines and challenges of NLP /NLP /IT / SNSCE\n",
      "NLP\n",
      "Natural Language Processing\n",
      "• Natural language processing, or NLP , is the field that involves getting systems to understand human\n",
      "languages.\n",
      "• Natural language processing (NLP) refers to the branch of computer science—and more specifically, the\n",
      "branch of artificial intelligence or AI—concerned with giving computers the ability to understand text and\n",
      "spoken words in much the same way human beings can.\n",
      "\n",
      " Match 2:\n",
      "• NLP combines computational linguistics—rule-based modeling of human language—with statistical, machine\n",
      "learning, and deep learning models. Together, these technologies enable computers to process human\n",
      "language in the form of text or voice data and to ‘understand’its full meaning, complete with the speaker or\n",
      "writer’sintent and sentiment\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Query: What are language models?\n",
      "\n",
      " Match 1:\n",
      "correction etc. The input to a language model is usually a training set of example sentences. The output is a\n",
      "probability distribution over sequences of words.\n",
      "Types of Language Model\n",
      "•Grammar-based models\n",
      "•Statistical models\n",
      "\n",
      " Match 2:\n",
      "8/919AD601 -Language Models /NLP /IT / SNSCE\n",
      "Language Model\n",
      "Applications of statistical language modeling\n",
      "1.Statistical language models are used to generate text in many similar natural language processing tasks,\n",
      "such as:\n",
      "2.Speech Recognization - V oiceassistants such as Siri and Alexa are examples of how language models\n",
      "help machines in processing speech audio.\n",
      "3.Machine Translation - Google Translator and Microsoft Translate are examples of how NLP models can\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Query: Explain tokenization in NLP.\n",
      "\n",
      " Match 1:\n",
      "2/919AD601 - Tokenization /NLP /IT / SNSCE\n",
      "Tokenization\n",
      "Tokenization is splitting the raw text into small chunks of words or sentences, called tokens.\n",
      "If the text is split into words, then its called as 'Word Tokenization' and if it's split into sentences then its\n",
      "called as 'Sentence Tokenization’.\n",
      "Generally 'space' is used to perform the word tokenization and characters like 'periods, exclamation point\n",
      "and newline char are used for Sentence Tokenization.\n",
      "Simple Approaches to Tokenization\n",
      "\n",
      " Match 2:\n",
      "5/816AD601 - Introduction to NL TK  /NLP /IT / SNSCE\n",
      "Introduction to NLTK\n",
      "Tokenization\n",
      "Tokenization is the process of breaking text up into smaller chunks as per our requirements.\n",
      "Word tokenization is the process of breaking a sentence into words. word_tokenize function has been used,\n",
      "which returns a list of words as output.\n",
      "Sentence tokenization is the process of breaking a corpus into sentence level tokens. It’s essentially used\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Query: What is NLTK used for?\n",
      "\n",
      " Match 1:\n",
      "3/816AD601 - Introduction to NL TK  /NLP /IT / SNSCE\n",
      "Introduction to NLTK\n",
      "NLTK (Natural Language Toolkit) is a toolkit build for working with NLP in Python. It provides us various\n",
      "text processing libraries with a lot of test datasets.\n",
      "A variety of tasks can be performed using NLTK such as tokenizing, parse tree visualization etc.\n",
      "NLTK is a standard python library with prebuilt functions and utilities for the ease of use and\n",
      "implementation.\n",
      "\n",
      " Match 2:\n",
      "2/816AD601 - Introduction to NL TK  /NLP /IT / SNSCE\n",
      "Introduction to NLTK\n",
      "Python and the Natural Language Toolkit (NLTK)\n",
      "The Python programing language provides a wide range of tools and libraries for attacking specific NLP\n",
      "tasks.\n",
      "Many of these are found in the Natural Language Toolkit, or NLTK, an open source collection of libraries,\n",
      "programs, and education resources for building NLP programs.\n",
      "Statistical NLP, machine learning, and deep learning\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Query: Describe different types of language models.\n",
      "\n",
      " Match 1:\n",
      "correction etc. The input to a language model is usually a training set of example sentences. The output is a\n",
      "probability distribution over sequences of words.\n",
      "Types of Language Model\n",
      "•Grammar-based models\n",
      "•Statistical models\n",
      "\n",
      " Match 2:\n",
      "8/919AD601 -Language Models /NLP /IT / SNSCE\n",
      "Language Model\n",
      "Applications of statistical language modeling\n",
      "1.Statistical language models are used to generate text in many similar natural language processing tasks,\n",
      "such as:\n",
      "2.Speech Recognization - V oiceassistants such as Siri and Alexa are examples of how language models\n",
      "help machines in processing speech audio.\n",
      "3.Machine Translation - Google Translator and Microsoft Translate are examples of how NLP models can\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load FAISS Vector Store with Safe Deserialization\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_store = FAISS.load_local(\"vector_store\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "# List of sample queries\n",
    "queries = [\n",
    "    \"What is NLP?\",\n",
    "    \"What are language models?\",\n",
    "    \"Explain tokenization in NLP.\",\n",
    "    \"What is NLTK used for?\",\n",
    "    \"Describe different types of language models.\"\n",
    "]\n",
    "\n",
    "# Run queries and display results\n",
    "for query in queries:\n",
    "    print(f\"\\n Query: {query}\")\n",
    "    results = vector_store.similarity_search(query, k=2)  # Fetch top 2 matches\n",
    "\n",
    "    for i, res in enumerate(results):\n",
    "        print(f\"\\n Match {i+1}:\")\n",
    "        print(res.page_content[:500])  \n",
    "    print(\"\\n\" + \"-\"*80)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b92ea2-029d-4133-bff4-c802fbf987a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
